PRISM
=====

Version: 4.7.dev
Date: Wed Apr 27 15:14:11 EDT 2022
Hostname: YEW
Memory limits: cudd=1g, java(heap)=1g
Command line: prism rocks2.prism -const N=6 samplerocks.props -prop 6 -ex

Parsing model file "rocks2.prism"...

Type:        POMDP
Modules:     master rock1 rock2 robot 
Variables:   started done r1qual r1taken r1lastobs r2qual r2taken r2lastobs x y 
Observables: started r1taken r1lastobs r2taken r2lastobs done seefinish seebad seegood atr1 atr2 nearr1 nearr2

Parsing properties file "samplerocks.props"...

6 properties:
(1) Rmin=? [ F "goal" ]
(2) Rmax=? [ F "goal" ]
(3) R{"rew"}max=? [ F "goal" ]
(4) R{"cos"}min=? [ F "goal" ]
(5) <<*>> (1*R{"rew"}max=? [ F "goal" ]+1*R{"cos"}min=? [ F "goal" ])
(6) R{"cost"}min=? [ F "goal" ]

---------------------------------------------------------------------

Model checking: R{"cost"}min=? [ F "goal" ]
Model constants: N=6

Building model...
Model constants: N=6

Computing reachable states... 816 states
Reachable states exploration and model construction done in 0.091 secs.
Sorting reachable states list...

Time for model construction: 0.113 seconds.

Type:        POMDP
States:      816 (1 initial)
Obs/unobs:   74/144
Transitions: 7325
Choices:     4295
Max/avg:     7/5.26
Building reward structure...
Calling Perseus pomdp solver
Starting Prob1 (max)...
Prob1 (max) took 18 iterations and 0.037 seconds.
endS 
762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815  
start Episode0 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=74(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=2,y=1)

 =============== Step 4 Cur state
Action = west reward = -1.0 states after action :
s=68(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 8 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -10.0 DiscountedReward -7.419017309247259
End Episode 0 out of 100
Current average undiscounted : -10.0; average discounted : -7.419017309247259
start Episode1 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = r2sense reward = -1.0 states after action :
s=98(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=true,x=0,y=1)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=68(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 8 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -10.0 DiscountedReward -7.419017309247259
End Episode 1 out of 100
Current average undiscounted : -10.0; average discounted : -7.419017309247259
start Episode2 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=291(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.2901693246074215
End Episode 2 out of 100
Current average undiscounted : -9.333333333333334; average discounted : -7.042734647700647
start Episode3 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=742(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.679954700653509
End Episode 3 out of 100
Current average undiscounted : -9.25; average discounted : -6.952039660938862
start Episode4 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=68(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.210754965232421
End Episode 4 out of 100
Current average undiscounted : -9.0; average discounted : -6.803782721797573
start Episode5 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=1(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=2(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=3(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=9(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=10(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=11(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = east reward = -1.0 states after action :
s=17(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=23(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=35(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 10 Cur state
Action = finish reward = -0.0 states after action :
s=766(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.395011805507813
End Episode 5 out of 100
Current average undiscounted : -8.833333333333334; average discounted : -6.73565423574928
start Episode6 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=742(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.679954700653509
End Episode 6 out of 100
Current average undiscounted : -8.857142857142858; average discounted : -6.727697159307027
start Episode7 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = south reward = -1.0 states after action :
s=737(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=5)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=743(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=5)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=755(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=5)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=815(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=5)
UndiscountedReward -9.0 DiscountedReward -6.679954700653509
End Episode 7 out of 100
Current average undiscounted : -8.875; average discounted : -6.721729351975338
start Episode8 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=742(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.679954700653509
End Episode 8 out of 100
Current average undiscounted : -8.88888888888889; average discounted : -6.71708772405069
start Episode9 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=1(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=2(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=8(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=9(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=10(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=11(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = east reward = -1.0 states after action :
s=17(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=23(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=29(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=35(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=766(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -7.025261215232422
End Episode 9 out of 100
Current average undiscounted : -8.9; average discounted : -6.747905073168863
start Episode10 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=286(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.8889062638458
End Episode 10 out of 100
Current average undiscounted : -8.909090909090908; average discounted : -6.760723363230403
start Episode11 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=1(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=2(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=8(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=10(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=11(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 6 Cur state
Action = north reward = -1.0 states after action :
s=10(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = north reward = -1.0 states after action :
s=9(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=15(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=2)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=27(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=2)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=33(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=2)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=764(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=2)
UndiscountedReward -9.0 DiscountedReward -7.025261215232422
End Episode 11 out of 100
Current average undiscounted : -8.916666666666666; average discounted : -6.78276818423057
start Episode12 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=1(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=2(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=8(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=9(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=11(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 6 Cur state
Action = east reward = -1.0 states after action :
s=17(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 7 Cur state
Action = east reward = -1.0 states after action :
s=23(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=29(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=35(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 10 Cur state
Action = finish reward = -0.0 states after action :
s=766(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.395011805507813
End Episode 12 out of 100
Current average undiscounted : -8.846153846153847; average discounted : -6.752940770482666
start Episode13 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=63(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.210754965232421
End Episode 13 out of 100
Current average undiscounted : -8.785714285714286; average discounted : -6.714213212964792
start Episode14 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=286(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.8889062638458
End Episode 14 out of 100
Current average undiscounted : -8.8; average discounted : -6.725859416356858
start Episode15 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=354(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=355(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 6 Cur state
Action = r1sample reward = -0.0 states after action :
s=664(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = north reward = -1.0 states after action :
s=663(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 8 Cur state
Action = north reward = -1.0 states after action :
s=662(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 9 Cur state
Action = r2sample reward = -0.0 states after action :
s=728(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=734(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=2)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=740(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=2)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=746(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=2)

 =============== Step 13 Cur state
Action = east reward = -1.0 states after action :
s=752(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=2)

 =============== Step 14 Cur state
Action = finish reward = -0.0 states after action :
s=812(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=2)
UndiscountedReward -10.0 DiscountedReward -7.295956965620833
End Episode 15 out of 100
Current average undiscounted : -8.875; average discounted : -6.761490513185857
start Episode16 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=358(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=2,y=1)

 =============== Step 4 Cur state
Action = west reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 8 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 10 Cur state
Action = south reward = -1.0 states after action :
s=731(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=5)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=737(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=5)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=743(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=5)

 =============== Step 13 Cur state
Action = east reward = -1.0 states after action :
s=749(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=5)

 =============== Step 14 Cur state
Action = east reward = -1.0 states after action :
s=755(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=5)

 =============== Step 15 Cur state
Action = finish reward = -0.0 states after action :
s=815(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=5)
UndiscountedReward -11.0 DiscountedReward -7.809299048900338
End Episode 16 out of 100
Current average undiscounted : -9.0; average discounted : -6.823126309404356
start Episode17 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=291(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.8889062638458
End Episode 17 out of 100
Current average undiscounted : -9.0; average discounted : -6.826780751317769
start Episode18 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=347(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = south reward = -1.0 states after action :
s=731(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=5)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=737(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=5)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=743(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=5)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=749(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=5)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=755(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=5)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=815(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=5)
UndiscountedReward -9.0 DiscountedReward -6.643200106122259
End Episode 18 out of 100
Current average undiscounted : -9.0; average discounted : -6.8171186120969525
start Episode19 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=347(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=742(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.679954700653509
End Episode 19 out of 100
Current average undiscounted : -9.0; average discounted : -6.8102604165247795
start Episode20 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=68(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.809491904470799
End Episode 20 out of 100
Current average undiscounted : -9.0; average discounted : -6.810223820712686
start Episode21 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = r2sense reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=74(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=2,y=1)

 =============== Step 5 Cur state
Action = west reward = -1.0 states after action :
s=68(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 7 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 8 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 9 Cur state
Action = south reward = -1.0 states after action :
s=137(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=5)

 =============== Step 10 Cur state
Action = north reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 13 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 14 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -11.0 DiscountedReward -7.9980664437848965
End Episode 21 out of 100
Current average undiscounted : -9.090909090909092; average discounted : -6.864216667215968
start Episode22 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = south reward = -1.0 states after action :
s=731(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=5)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=737(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=5)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=743(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=5)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=749(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=5)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=755(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=5)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=815(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=5)
UndiscountedReward -9.0 DiscountedReward -6.643200106122259
End Episode 22 out of 100
Current average undiscounted : -9.08695652173913; average discounted : -6.854607251516241
start Episode23 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=291(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=295(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=5)

 =============== Step 7 Cur state
Action = north reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -10.0 DiscountedReward -7.49446095065351
End Episode 23 out of 100
Current average undiscounted : -9.125; average discounted : -6.881267822313628
start Episode24 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=291(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.8889062638458
End Episode 24 out of 100
Current average undiscounted : -9.12; average discounted : -6.881573359974915
start Episode25 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=68(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=70(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=71(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 6 Cur state
Action = north reward = -1.0 states after action :
s=70(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = north reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 8 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=140(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=2)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=146(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=2)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=152(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=2)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=158(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=2)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=776(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=2)
UndiscountedReward -10.0 DiscountedReward -7.49446095065351
End Episode 25 out of 100
Current average undiscounted : -9.153846153846153; average discounted : -6.9051459596164
start Episode26 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = r2sense reward = -1.0 states after action :
s=320(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=true,x=0,y=0)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=286(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 5 Cur state
Action = east reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=295(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=5)

 =============== Step 8 Cur state
Action = north reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 13 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 14 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -11.0 DiscountedReward -8.069737903120833
End Episode 26 out of 100
Current average undiscounted : -9.222222222222221; average discounted : -6.948278994561008
start Episode27 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=286(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.8889062638458
End Episode 27 out of 100
Current average undiscounted : -9.214285714285714; average discounted : -6.946158539892608
start Episode28 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = r1sense reward = -1.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=291(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -10.0 DiscountedReward -7.49446095065351
End Episode 28 out of 100
Current average undiscounted : -9.241379310344827; average discounted : -6.9650655195740185
start Episode29 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=63(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.809491904470799
End Episode 29 out of 100
Current average undiscounted : -9.233333333333333; average discounted : -6.959879732403911
start Episode30 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=286(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 6 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.251480277732421
End Episode 30 out of 100
Current average undiscounted : -9.193548387096774; average discounted : -6.937028137091927
start Episode31 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=63(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.809491904470799
End Episode 31 out of 100
Current average undiscounted : -9.1875; average discounted : -6.933042629822517
start Episode32 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=358(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=2,y=1)

 =============== Step 4 Cur state
Action = west reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 8 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 10 Cur state
Action = south reward = -1.0 states after action :
s=731(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=5)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=737(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=5)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=743(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=5)

 =============== Step 13 Cur state
Action = east reward = -1.0 states after action :
s=755(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=5)

 =============== Step 14 Cur state
Action = finish reward = -0.0 states after action :
s=815(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=5)
UndiscountedReward -10.0 DiscountedReward -7.295956965620833
End Episode 32 out of 100
Current average undiscounted : -9.212121212121213; average discounted : -6.944040033937617
start Episode33 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=1(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=2(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=8(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=9(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=10(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=11(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = east reward = -1.0 states after action :
s=17(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=23(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=29(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=35(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=766(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -7.025261215232422
End Episode 33 out of 100
Current average undiscounted : -9.205882352941176; average discounted : -6.946428892210995
start Episode34 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = r1sense reward = -1.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=286(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.2901693246074215
End Episode 34 out of 100
Current average undiscounted : -9.17142857142857; average discounted : -6.9276786188508925
start Episode35 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = r1sense reward = -1.0 states after action :
s=448(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=true,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=291(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -10.0 DiscountedReward -7.49446095065351
End Episode 35 out of 100
Current average undiscounted : -9.194444444444445; average discounted : -6.943422572512077
start Episode36 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=63(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.809491904470799
End Episode 36 out of 100
Current average undiscounted : -9.18918918918919; average discounted : -6.9398028247271775
start Episode37 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=1(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=3(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=9(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=10(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=11(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 6 Cur state
Action = east reward = -1.0 states after action :
s=17(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 7 Cur state
Action = east reward = -1.0 states after action :
s=23(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=29(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=35(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 10 Cur state
Action = finish reward = -0.0 states after action :
s=766(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.395011805507813
End Episode 37 out of 100
Current average undiscounted : -9.157894736842104; average discounted : -6.925466218958246
start Episode38 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=742(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.679954700653509
End Episode 38 out of 100
Current average undiscounted : -9.153846153846153; average discounted : -6.919171051822228
start Episode39 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=286(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.8889062638458
End Episode 39 out of 100
Current average undiscounted : -9.15; average discounted : -6.918414432122818
start Episode40 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=286(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 6 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.251480277732421
End Episode 40 out of 100
Current average undiscounted : -9.121951219512194; average discounted : -6.902147745430368
start Episode41 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=68(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = south reward = -1.0 states after action :
s=137(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=5)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=149(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=5)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=155(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=5)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=161(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=5)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=779(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=5)
UndiscountedReward -9.0 DiscountedReward -6.809491904470799
End Episode 41 out of 100
Current average undiscounted : -9.119047619047619; average discounted : -6.89994165397895
start Episode42 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=63(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 4 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.167886215232421
End Episode 42 out of 100
Current average undiscounted : -9.093023255813954; average discounted : -6.882917108891823
start Episode43 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=68(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.210754965232421
End Episode 43 out of 100
Current average undiscounted : -9.068181818181818; average discounted : -6.8676406965359265
start Episode44 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.111154608377049
End Episode 44 out of 100
Current average undiscounted : -9.044444444444444; average discounted : -6.85082989457684
start Episode45 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = r2sense reward = -1.0 states after action :
s=381(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=true,x=0,y=0)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=358(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=2,y=1)

 =============== Step 5 Cur state
Action = west reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 7 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 8 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 9 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 10 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=742(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 13 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 14 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 15 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -11.0 DiscountedReward -7.881159117339792
End Episode 45 out of 100
Current average undiscounted : -9.08695652173913; average discounted : -6.873228355941252
start Episode46 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=1(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=2(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=8(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=9(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=10(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=11(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = east reward = -1.0 states after action :
s=17(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=23(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=29(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=35(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=766(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -7.025261215232422
End Episode 46 out of 100
Current average undiscounted : -9.085106382978724; average discounted : -6.8764630976282985
start Episode47 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=291(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 6 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = north reward = -1.0 states after action :
s=602(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 8 Cur state
Action = north reward = -1.0 states after action :
s=601(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=607(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=2)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=619(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=2)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=625(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=2)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=800(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=2)
UndiscountedReward -9.0 DiscountedReward -6.850217216970799
End Episode 47 out of 100
Current average undiscounted : -9.083333333333334; average discounted : -6.875916308447934
start Episode48 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=68(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=70(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=71(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 6 Cur state
Action = north reward = -1.0 states after action :
s=70(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = north reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 8 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=140(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=2)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=146(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=2)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=152(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=2)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=158(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=2)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=776(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=2)
UndiscountedReward -10.0 DiscountedReward -7.49446095065351
End Episode 48 out of 100
Current average undiscounted : -9.10204081632653; average discounted : -6.888539668492945
start Episode49 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=347(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 4 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=742(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.0315312638458
End Episode 49 out of 100
Current average undiscounted : -9.08; average discounted : -6.871399500400003
start Episode50 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -7.0 DiscountedReward -5.475663074607421
End Episode 50 out of 100
Current average undiscounted : -9.03921568627451; average discounted : -6.844032119502109
start Episode51 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=347(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=742(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.074400013845799
End Episode 51 out of 100
Current average undiscounted : -9.01923076923077; average discounted : -6.829231502085642
start Episode52 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=1(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=2(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=3(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=9(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=11(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 6 Cur state
Action = east reward = -1.0 states after action :
s=23(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 7 Cur state
Action = east reward = -1.0 states after action :
s=29(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=35(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 9 Cur state
Action = finish reward = -0.0 states after action :
s=766(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -7.0 DiscountedReward -5.73159137421875
End Episode 52 out of 100
Current average undiscounted : -8.981132075471699; average discounted : -6.808521310993813
start Episode53 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=1(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=2(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=8(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=9(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=10(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=12(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=5)

 =============== Step 7 Cur state
Action = north reward = -1.0 states after action :
s=11(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=17(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=23(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=29(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=35(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=766(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -10.0 DiscountedReward -7.6239981544708
End Episode 53 out of 100
Current average undiscounted : -9.0; average discounted : -6.823622734021165
start Episode54 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=1(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=2(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=8(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=9(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=10(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=11(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = east reward = -1.0 states after action :
s=17(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=23(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=29(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=35(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=766(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -7.025261215232422
End Episode 54 out of 100
Current average undiscounted : -9.0; average discounted : -6.827288888225006
start Episode55 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=68(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.210754965232421
End Episode 55 out of 100
Current average undiscounted : -8.982142857142858; average discounted : -6.816279353885853
start Episode56 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=63(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.210754965232421
End Episode 56 out of 100
Current average undiscounted : -8.964912280701755; average discounted : -6.805656118997196
start Episode57 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=1(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=2(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=8(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=9(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=10(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=11(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = east reward = -1.0 states after action :
s=17(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=23(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=29(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=35(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=766(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -7.025261215232422
End Episode 57 out of 100
Current average undiscounted : -8.96551724137931; average discounted : -6.809442413759872
start Episode58 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=358(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=2,y=1)

 =============== Step 4 Cur state
Action = west reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 8 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=742(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 13 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 14 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -10.0 DiscountedReward -7.295956965620833
End Episode 58 out of 100
Current average undiscounted : -8.983050847457626; average discounted : -6.817688423113449
start Episode59 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = south reward = -1.0 states after action :
s=731(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=5)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=737(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=5)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=743(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=5)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=755(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=5)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=815(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=5)
UndiscountedReward -9.0 DiscountedReward -6.679954700653509
End Episode 59 out of 100
Current average undiscounted : -8.983333333333333; average discounted : -6.81539286107245
start Episode60 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=291(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.8889062638458
End Episode 60 out of 100
Current average undiscounted : -8.98360655737705; average discounted : -6.816597998822832
start Episode61 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=63(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 4 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.167886215232421
End Episode 61 out of 100
Current average undiscounted : -8.96774193548387; average discounted : -6.806134905539117
start Episode62 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=1(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=2(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = r2sense reward = -1.0 states after action :
s=2(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=8(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=10(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=11(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = north reward = -1.0 states after action :
s=10(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 8 Cur state
Action = north reward = -1.0 states after action :
s=9(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=15(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=2)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=21(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=2)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=27(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=2)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=33(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=2)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=764(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=2)
UndiscountedReward -11.0 DiscountedReward -8.192798246747259
End Episode 62 out of 100
Current average undiscounted : -9.0; average discounted : -6.828145434764642
start Episode63 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=742(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.679954700653509
End Episode 63 out of 100
Current average undiscounted : -9.0; average discounted : -6.825829954544156
start Episode64 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=286(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.8889062638458
End Episode 64 out of 100
Current average undiscounted : -9.0; average discounted : -6.826800359302643
start Episode65 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=68(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.809491904470799
End Episode 65 out of 100
Current average undiscounted : -9.0; average discounted : -6.826538109987009
start Episode66 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = r2sense reward = -1.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=347(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 5 Cur state
Action = east reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 8 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=742(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 13 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 14 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -10.0 DiscountedReward -7.295956965620833
End Episode 66 out of 100
Current average undiscounted : -9.014925373134329; average discounted : -6.833544361563634
start Episode67 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=742(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.679954700653509
End Episode 67 out of 100
Current average undiscounted : -9.014705882352942; average discounted : -6.831285690079661
start Episode68 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=347(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 4 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=742(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.0315312638458
End Episode 68 out of 100
Current average undiscounted : -9.0; average discounted : -6.8196950462212
start Episode69 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=286(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.8889062638458
End Episode 69 out of 100
Current average undiscounted : -9.0; average discounted : -6.820683777901551
start Episode70 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=1(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = r1sense reward = -1.0 states after action :
s=163(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=true,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=2(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=8(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=9(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=10(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=11(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=17(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=23(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=29(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=35(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=766(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -10.0 DiscountedReward -7.6239981544708
End Episode 70 out of 100
Current average undiscounted : -9.014084507042254; average discounted : -6.831998064895485
start Episode71 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=347(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=742(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.679954700653509
End Episode 71 out of 100
Current average undiscounted : -9.01388888888889; average discounted : -6.8298863515032355
start Episode72 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=68(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.210754965232421
End Episode 72 out of 100
Current average undiscounted : -9.0; average discounted : -6.821405099636512
start Episode73 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = r1sense reward = -1.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=68(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 8 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -10.0 DiscountedReward -7.419017309247259
End Episode 73 out of 100
Current average undiscounted : -9.013513513513514; average discounted : -6.829480940306927
start Episode74 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=63(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 4 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 11 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.167886215232421
End Episode 74 out of 100
Current average undiscounted : -9.0; average discounted : -6.820659677305934
start Episode75 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=347(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 4 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=736(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=742(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=748(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=754(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=814(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -8.0 DiscountedReward -6.0315312638458
End Episode 75 out of 100
Current average undiscounted : -8.986842105263158; average discounted : -6.810276408707774
start Episode76 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=68(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.809491904470799
End Episode 76 out of 100
Current average undiscounted : -8.987012987012987; average discounted : -6.8102662203410596
start Episode77 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = r1sense reward = -1.0 states after action :
s=449(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=true,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=291(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -10.0 DiscountedReward -7.49446095065351
End Episode 77 out of 100
Current average undiscounted : -9.0; average discounted : -6.819037947652758
start Episode78 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=345(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=346(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=352(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=353(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=419(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=420(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=421(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = r1sample reward = -0.0 states after action :
s=730(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 9 Cur state
Action = south reward = -1.0 states after action :
s=731(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=5)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=737(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=5)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=743(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=5)

 =============== Step 12 Cur state
Action = east reward = -1.0 states after action :
s=755(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=5)

 =============== Step 13 Cur state
Action = finish reward = -0.0 states after action :
s=815(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=5)
UndiscountedReward -9.0 DiscountedReward -6.679954700653509
End Episode 78 out of 100
Current average undiscounted : -9.0; average discounted : -6.817277400222388
start Episode79 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=284(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=285(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = east reward = -1.0 states after action :
s=291(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=1)

 =============== Step 4 Cur state
Action = south reward = -1.0 states after action :
s=292(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = south reward = -1.0 states after action :
s=293(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=3)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=294(started=true,done=false,r1qual=true,r1taken=false,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 7 Cur state
Action = r1sample reward = -0.0 states after action :
s=603(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=609(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=615(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=621(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=627(started=true,done=false,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=802(started=true,done=true,r1qual=true,r1taken=true,r1lastobs=false,r2qual=false,r2taken=false,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.8889062638458
End Episode 79 out of 100
Current average undiscounted : -9.0; average discounted : -6.8181727610176805
start Episode80 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = south reward = -1.0 states after action :
s=62(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=1)

 =============== Step 3 Cur state
Action = south reward = -1.0 states after action :
s=63(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=2)

 =============== Step 4 Cur state
Action = east reward = -1.0 states after action :
s=69(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=1,y=2)

 =============== Step 5 Cur state
Action = r2sample reward = -0.0 states after action :
s=134(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=2)

 =============== Step 6 Cur state
Action = south reward = -1.0 states after action :
s=135(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=3)

 =============== Step 7 Cur state
Action = south reward = -1.0 states after action :
s=136(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=1,y=4)

 =============== Step 8 Cur state
Action = east reward = -1.0 states after action :
s=142(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=2,y=4)

 =============== Step 9 Cur state
Action = east reward = -1.0 states after action :
s=148(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=3,y=4)

 =============== Step 10 Cur state
Action = east reward = -1.0 states after action :
s=154(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=4,y=4)

 =============== Step 11 Cur state
Action = east reward = -1.0 states after action :
s=160(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)

 =============== Step 12 Cur state
Action = finish reward = -0.0 states after action :
s=778(started=true,done=true,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=true,r2lastobs=false,x=5,y=4)
UndiscountedReward -9.0 DiscountedReward -6.809491904470799
End Episode 80 out of 100
Current average undiscounted : -9.0; average discounted : -6.8180655899492
start Episode81 out of 100
start running episode

 =============== Step 1 Cur state
Action = placement reward = -0.0 states after action :
s=61(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=false,x=0,y=0)

 =============== Step 2 Cur state
Action = r2sense reward = -1.0 states after action :
s=97(started=true,done=false,r1qual=false,r1taken=false,r1lastobs=false,r2qual=true,r2taken=false,r2lastobs=true,x=0,y=0)
